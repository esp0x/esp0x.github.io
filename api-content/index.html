{"posts":[{"title":"使用prometheus监控主机docker容器运行状态","content":"部署方式：容器化部署 部署流程： # 运行node-exporter监控主机基本信息，访问9100端口进行验证 docker run -d --name node \\ -p 9100:9100 \\ -v /proc:/host/proc \\ -v /sys:/host/sys \\ -v /:/rootfs \\ --net=host prom/node-exporter \\ --path.procfs /host/proc \\ --path.sysfs /host/sys \\ --collector.filesystem.ignored-mount-points &quot;^/(sys|proc|dev|host|etc)($|/)&quot; # 运行cAdvisor监控容器运行状态，访问8080端口进行验证 docker run -v /:/rootfs:ro \\ -v /var/run:/var/run/:rw \\ -v /sys:/sys:ro \\ -v /var/lib/docker:/var/lib/docker:ro \\ -p 8080:8080 \\ --detach=true \\ --name=cadvisor \\ --net=host google/cadvisor # 在监控主节点上部署prometheus server，客户节点无需安装 docker run -d -p 9090:9090 --name prometheus --net=host prom/prometheus # 将容器中的配置文件拷贝到宿主，进行修改 docker cp prometheus:/etc/prometheus/prometheus.yml ./ # 修改static_configs，指向所有客户节点的9100和8080端口获取数据 vim prometheus.yml # 配置更新完成后，删除原prometheus server容器 docker rm prometheus -f # 重新将本地配置文件映射到容器中，访问9090端口进行验证 docker run -d -p 9090:9090 --name prometheus --net=host -v /root/prometheus.yml:/etc/prometheus/prometheus.yml prom/prometheus # 部署grafana图形化界面，访问3000端口进行验证 mkdir grafana-storage chmod 777 -R grafana-storage/ docker run -d -p 3000:3000 \\ --name grafana \\ -v /root/grafana-storage:/var/lib/grafana \\ -e &quot;GF_SECURITY_ADMIN_PASSWORD=123456&quot; grafana/grafana # grafana模板，打开grafana.com, 点击dashborads，根据数据源和类型选择合适的模板 ","link":"https://esp0x.github.io/post/shi-yong-prometheus-jian-kong-zhu-ji-docker-rong-qi-yun-xing-zhuang-tai/"},{"title":"如何拆除Linux软Raid设备","content":"1. 卸载设备 # umount /dev/md0 2. 停止raid设备 # mdadm -S /dev/md0 // 停止raid设备 3. 查看属于raid组的设备名称 # blkid 4. 从raid组中删除硬盘 # mdadm --misc --zero-superblock /dev/xxx 以下操作可选： 5. 删除配置文件：rm -f /etc/mdadm.conf 6. 更新/etc/fstab ","link":"https://esp0x.github.io/post/ru-he-chai-chu-linux-ruan-raid-she-bei/"},{"title":"Nmap工具常用使用场景","content":"主机发现 以192.168.1.0/24举例 nmap -PE 192.168.1.0/24 # icmp namp -PO 192.168.1.0/24 # ip nmap -PS 192.168.1.0/24 # tcp syn nmap -PA 192.168.1.0/24 # tcp ack nmap -PU 192.168.1.0/24 # udp nmap -PY 192.168.1.0/24 # sctp 端口扫描 1.SYN Scanning nmap -sS 192.168.1.0/24 # 仅发送SYN，返回SYN/ACK应答表示端口开启，返回RST表示端口关闭 2.TCP Scanning nmap -sT 192.168.1.0/24 # 建立完整的TCP连接，表示端口开启，否则表示关闭 3.UDP Scanning nmap -sU 192.168.1.0/24 # UDP，无应答，表示端口开启；返回&quot;Port Unreachable&quot;信息，表示关闭 4.FIN Scanning nmap -sF 192.168.1.0/24 # 在TCP数据包中重置FIN标志位，无应答，表示开启；返回RST，表示端口关闭 5.NULL Scanning nmap -sN 192.168.1.0/24 # 在TCP数据包中不包含任何标志位，无应答，表示开启；返回RST，表示端口关闭 6.Xmas Scanning nmap -sX 192.168.1.0/24 # 在TCP数据包中重置FIN、RST、PUSH标志位，无应答，表示开启；返回RST，表示端口关闭 7.IDLE Scanning nmap -sI 172.16.1.1 192.168.1.0/24 # 利用僵尸主机进行扫描，假设僵尸IP为172.16.1.1，当僵尸机返回序列ID增加数量为2时，表示开启，为1时关闭 8.指定端口扫描 nmap -p 80,443 192.168.1.0/24 # 可以指定多个端口 9.扫描常见的100个端口 nmap -F 192.168.1.0/24 # 快速模式 10.使用协议名进行扫描 nmap -p http 192.168.1.0/24 nmap -p smtp 192.168.1.0/24 11.扫描常用端口 nmap --top-ports &lt;端口数量&gt; 192.168.1.0/24 操作系统指纹识别 nmap -O 192.168.1.0/24 # os nmap -sV 192.168.1.0/24 # service version nmap -A 192.168.1.0/24 # all 使用脚本 nmap --script &lt;脚本名称&gt; 192.168.1.0/24 ","link":"https://esp0x.github.io/post/nmap-gong-ju-chang-yong-shi-yong-chang-jing/"},{"title":"XFS文件系统修复流程","content":"详细流程如下： 1.使用 xfs_repair -n 执行文件系统错误检测，和 fsck -n 类似； 2.使用 xfs_repair 尝试进行修复； 3.当遇见无法正常挂载文件系统时，需要使用强制模式 -L 模式进行修复，会丢失部分数据，可以先对metadata进行模拟修复测试： xfs_metadump [partition] /path/to/file.metadump # 对需要修复的分区进行dump xfs_mdrestore /path/to/file.metadump /path/to/file.img # 生成img文件 losetup --show --find /path/to/file.img # 使用lostup工具将数据放到/dev/loop0 xfs_repair -L /dev/loop0 # 尝试模拟修复 mount /dev/loop0 /mnt check the damage (note, this is an image of file system layout, but not the actual data) 4.拷贝真实数据，并对拷贝的数据进行修复： ddrescue -f -n [partition] /path/to/rescued.img rescue.log # 这需要一个比原分区大小更大的磁盘 ddrescue -d -f -r3 [partition] /path/to/rescued.img rescue.log losetup --show --find /path/to/rescued.img xfs_repair -L /dev/loop0 mount /dev/loop0 /mnt check the damage 5.如果真实环境缺少这样的备份分区，那么直接使用xfs_repair -L模式进行强制修复。 ","link":"https://esp0x.github.io/post/xfs-wen-jian-xi-tong-xiu-fu-liu-cheng/"},{"title":"ipmitool工具的使用说明","content":"安装工具 yum install -y ipmitool 启动服务 service ipmi start ipmitool -I open shell # 进入交互式shell 一、开关机，重启 1. 查看开关机状态： ipmitool -H (BMC的管理IP地址) -I lanplus -U (BMC登录用户名) -P (BMC 登录用户名的密码) power status 2. 开机： ipmitool -H (BMC的管理IP地址) -I lanplus -U (BMC登录用户名) -P (BMC 登录用户名的密码) power on 3. 关机： ipmitool -H (BMC的管理IP地址) -I lanplus -U (BMC登录用户名) -P (BMC 登录用户名的密码) power off 4. 重启： ipmitool -H (BMC的管理IP地址) -I lanplus -U (BMC登录用户名) -P (BMC 登录用户名的密码) power reset 二、用户管理 # 说明：[ChannelNo] 字段是可选的，ChannoNo为1或者8；BMC默认有2个用户：user id为1的匿名用户，user id为2的ADMIN用户；&lt;&gt;字段为必选内容；&lt;privilege level&gt;：2为user权限，3为Operator权限，4为Administrator权限； 1. 查看用户信息： ipmitool -H (BMC的管理IP地址) -I lanplus -U (BMC登录用户名) -P (BMC 登录用户名的密码) user list [ChannelNo] 2. 增加用户： ipmitool -H (BMC的管理IP地址) -I lanplus -U (BMC登录用户名) -P (BMC 登录用户名的密码) user set name &lt;user id&gt; &lt;username&gt; 3. 设置密码： ipmitool -H (BMC的管理IP地址) -I lanplus -U (BMC登录用户名) -P (BMC 登录用户名的密码) user set password &lt;user id&gt; &lt;password&gt; 4. 设置用户权限： ipmitool -H (BMC的管理IP地址) -I lanplus -U (BMC登录用户名) -P (BMC 登录用户名的密码) user priv &lt;user id&gt; &lt;privilege level&gt; [ChannelNo] 5. 启用/禁用用户： ipmitool -H (BMC的管理IP地址) -I lanplus -U (BMC登录用户名) -P (BMC 登录用户名的密码) user enable/disable &lt;user id&gt; 三、IP网络设置 # 说明：[ChannelNo] 字段是可选的，ChannoNo为1(Share Nic网络)或者8（BMC独立管理网络）；设置网络参数，必须首先设置IP为静态，然后再进行其他设置； 1. 查看网络信息： ipmitool -H (BMC的管理IP地址) -I lanplus -U (BMC登录用户名) -P (BMC 登录用户名的密码) lan print [ChannelNo] 2. 修改IP为静态还是DHCP模式： ipmitool -H (BMC的管理IP地址) -I lanplus -U (BMC登录用户名) -P (BMC 登录用户名的密码) lan set &lt;ChannelNo&gt; ipsrc &lt;static/dhcp&gt; 3. 修改IP地址： ipmitool -H (BMC的管理IP地址) -I lanplus -U (BMC登录用户名) -P (BMC 登录用户名的密码) lan set &lt;ChannelNo&gt; ipaddr &lt;IPAddress&gt; 4. 修改子网掩码： ipmitool -H (BMC的管理IP地址) -I lanplus -U (BMC登录用户名) -P (BMC 登录用户名的密码) lan set &lt;ChannelNo&gt; netmask &lt;NetMask&gt; 5. 修改默认网关： ipmitool -H (BMC的管理IP地址) -I lanplus -U (BMC登录用户名) -P (BMC 登录用户名的密码) lan set &lt;ChannelNo&gt; defgw ipaddr &lt;默认网关&gt; 四、SOL功能 # 说明：&lt;9.6/19.2/38.4/57.6/115.2&gt;其中115.2代表115200，即*1000是表示的波特率; 1. 设置SOL串口波特率： ipmitool -H (BMC的管理IP地址) -I lanplus -U (BMC登录用户名) -P (BMC 登录用户名的密码) sol set volatile-bit-rate &lt;9.6/19.2/38.4/57.6/115.2&gt; 2. 打开SOL功能： ipmitool -H (BMC的管理IP地址) -I lanplus -U (BMC登录用户名) -P (BMC 登录用户名的密码) sol activate 3. 关闭SOL功能： ipmitool -H (BMC的管理IP地址) -I lanplus -U (BMC登录用户名) -P (BMC 登录用户名的密码) sol deactivate 五、SEL日志查看 1. 查看SEL日志： ipmitool -H (BMC的管理IP地址) -I lanplus -U (BMC登录用户名) -P (BMC 登录用户名的密码) sel list 六、FRU信息查看 1. 查看FRU信息： ipmitool -H (BMC的管理IP地址) -I lanplus -U (BMC登录用户名) -P (BMC 登录用户名的密码) fru list 七、SDR，Sensor信息查看 1. 查看SDR Sensor信息： ipmitool -H (BMC的管理IP地址) -I lanplus -U (BMC登录用户名) -P (BMC 登录用户名的密码) sdr 2. 查看Sensor信息： ipmitool -H (BMC的管理IP地址) -I lanplus -U (BMC登录用户名) -P (BMC 登录用户名的密码) sensor list 八、mc(管理单元BMC)状态和控制 1. 重启动BMC： ipmitool -H (BMC的管理IP地址) -I lanplus -U (BMC登录用户名) -P (BMC 登录用户名的密码) mc reset &lt;warm/cold&gt; 九、设置BMC的iptables防火墙 1. 设置某一段IP可以访问BMC ipmitool -H (BMC的管理IP地址) -I lanplus -U (BMC登录用户名) -P (BMC 登录用户名的密码) raw 0x32 0x76 0x01 0x01 ip1(0xa 0xa 0xa 0xa) ip2(0xb 0xb 0xb 0xb) ipmitool -H (BMC的管理IP地址) -I lanplus -U (BMC登录用户名) -P (BMC 登录用户名的密码) raw 0x32 0x76 0x09 2. 设置某个IP可以访问BMC ipmitool -H (BMC的管理IP地址) -I lanplus -U (BMC登录用户名) -P (BMC 登录用户名的密码) raw 0x32 0x76 0x00 0x01 ip1(0xa 0xa 0xa 0xa) ipmitool -H (BMC的管理IP地址) -I lanplus -U (BMC登录用户名) -P (BMC 登录用户名的密码) raw 0x32 0x76 0x09 3. 取消设置 ipmitool -H (BMC的管理IP地址) -I lanplus -U (BMC登录用户名) -P (BMC 登录用户名的密码) raw 0x32 0x76 0x08 4．获取防火墙设置 ipmitool -H (BMC的管理IP地址) -I lanplus -U (BMC登录用户名) -P (BMC 登录用户名的密码) raw 0x32 0x77 0x01 0x00 5. 阻止/开启某个端口 ipmitool -H (BMC的管理IP地址) -I lanplus -U (BMC登录用户名) -P (BMC 登录用户名的密码) raw 0x32 0x76 0x02 0x00/0x01 0x00 (portno)0x22 0x00 6. 取消某个端口的设置（6是5的对应取消操作） ipmitool -H (BMC的管理IP地址) -I lanplus -U (BMC登录用户名) -P (BMC 登录用户名的密码) raw 0x32 0x76 0x06 0x00/0x01 0x00 (portno)0x22 0x00 参考文献 https://www.cnblogs.com/EricDing/p/8995263.html ","link":"https://esp0x.github.io/post/ipmitool-gong-ju-de-shi-yong-shuo-ming/"},{"title":"NVIDIA显卡常用命令","content":"GPU日志收集 # nvidia-bug-report.sh // 执行后输出nvidia-bug-report.log.gz文件 驱动问题常见解决方法 维持较新的驱动版本 禁用nouveau模块 打开GPU驱动常驻内存模式并配置开机自启动 GPU故障后，可尝试重启主机解决 禁用nouveau模块 # lsmod | grep -i nouveau // 如果有输出，表示启动状态，否则为禁用状态 # CentOS 7 # 编辑或新建 blacklist-nouveau.conf 文件 [root@zj ~]# vim /usr/lib/modprobe.d/blacklist-nouveau.conf blacklist nouveau options nouveau modeset=0 # 执行如下命令并重启系统使内核生效 [root@zj ~]# dracut -f [root@zj ~]# shutdown -ry 0 # ubuntu # vi /etc/modprobe.d/blacklist.conf 最后一行加入 blacklist nouveau # update-initramfs -u # reboot GPU驱动内存常驻模式 # nvidia-smi -pm 1 GPU加载数量和ERR检查 # 以下两个命令显示的GPU卡数量需要保持一致，可用于判断是否有GPU离线 # lspci | grep -i nvidia # nvidia-smi # 检查输出中是否包含ERR错误字样，可用于实现健康检查 GPU常用性能指标获取 # nvidia-smi \\ &gt; --query-gpu=memory.total,memory.used,memory.free,utilization.memory,utilization.gpu,temperature.gpu,fan.speed \\ &gt; --format=csv,noheader,nounits # nvidia-smi --help-query-gpu // 查看可用的查询参数 ","link":"https://esp0x.github.io/post/nvidia-xian-qia-chang-yong-ming-ling/"},{"title":"理解OAuth 2.0的基本流程","content":"快递员场景 需求：有没有一种办法，让快递员能够自由出入小区，又不用知道小区居民的密码，而且他的唯一权限就是送货，其他需要密码的场合，他都没有权限？ 授权机制的设计 第一步，门禁系统的密码输入器下面，增加一个按钮，叫做&quot;获取授权&quot;。快递员需要首先按这个按钮，去申请授权。 第二步，他按下按钮以后，屋主（也就是我）的手机就会跳出对话框：有人正在要求授权。系统还会显示该快递员的姓名、工号和所属的快递公司。 我确认请求属实，就点击按钮，告诉门禁系统，我同意给予他进入小区的授权。 第三步，门禁系统得到我的确认以后，向快递员显示一个进入小区的令牌（access token）。令牌就是类似密码的一串数字，只在短期内（比如七天）有效。 第四步，快递员向门禁系统输入令牌，进入小区。 有人可能会问，为什么不是远程为快递员开门，而要为他单独生成一个令牌？这是因为快递员可能每天都会来送货，第二天他还可以复用这个令牌。另外，有的小区有多重门禁，快递员可以使用同一个令牌通过它们。 转换理解 所以，OAuth就是一种授权机制，数据的所有者告诉系统，同意授权第三方应用进入系统，获取这些数据。系统从而产生一个短期的进入令牌（Token），用于替代密码，以供第三方使用。 关于Token 只需要记住三点：1、令牌的有效期是有限的，为了安全；2、令牌的权限范围一般很小；3、令牌可以被撤销； 参考文献 http://www.ruanyifeng.com/blog/2019/04/oauth_design.html ","link":"https://esp0x.github.io/post/li-jie-oauth-20-de-ji-ben-liu-cheng/"},{"title":"Nginx配置文件详解","content":"######Nginx配置文件nginx.conf中文详解##### #定义Nginx运行的用户和用户组 user www www; #nginx进程数，建议设置为等于CPU总核心数。 worker_processes 8; #全局错误日志定义类型，[ debug | info | notice | warn | error | crit ] error_log /usr/local/nginx/logs/error.log info; #进程pid文件 pid /usr/local/nginx/logs/nginx.pid; #指定进程可以打开的最大描述符：数目 #工作模式与连接数上限 #这个指令是指当一个nginx进程打开的最多文件描述符数目，理论值应该是最多打开文件数（ulimit -n）与nginx进程数相除，但是nginx分配请求并不是那么均匀，所以最好与ulimit -n 的值保持一致。 #现在在linux 2.6内核下开启文件打开数为65535，worker_rlimit_nofile就相应应该填写65535。 #这是因为nginx调度时分配请求到进程并不是那么的均衡，所以假如填写10240，总并发量达到3-4万时就有进程可能超过10240了，这时会返回502错误。 worker_rlimit_nofile 65535; events { use epoll; #单个进程最大连接数（最大连接数=连接数*进程数） worker_connections 65535; #keepalive超时时间。 keepalive_timeout 60; #分页大小可以用命令getconf PAGESIZE 取得。 #getconf PAGESIZE 设置为该值的整数倍 client_header_buffer_size 4k; #这个将为打开文件指定缓存，默认是没有启用的，max指定缓存数量，建议和打开文件数一致，inactive是指经过多长时间文件没被请求后删除缓存。 open_file_cache max=65535 inactive=60s; #语法:open_file_cache_valid time #默认值 60 #使用字段:http, server, location #这个指令指定了何时需要检查open_file_cache中缓存项目的有效信息. open_file_cache_valid 80s; #语法:open_file_cache_min_uses number #默认值: 1 #使用字段:http, server, location #这个指令指定了在open_file_cache指令无效的参数中一定的时间范围内可以使用的最小文件数,如果使用更大的值,文件描述符在cache中总是打开状态. open_file_cache_min_uses 1; #语法:open_file_cache_errors on | off #默认值: off #使用字段:http, server, location #这个指令指定是否在搜索一个文件是记录cache错误. open_file_cache_errors on; } #设定http服务器，利用它的反向代理功能提供负载均衡支持 http { #文件扩展名与文件类型映射表 include mime.types; #默认文件类型 default_type application/octet-stream; #默认编码 #charset utf-8; #服务器名字的hash表大小 #保存服务器名字的hash表是由指令server_names_hash_max_size 和server_names_hash_bucket_size所控制的。参数hash bucket size总是等于hash表的大小，并且是一路处理器缓存大小的倍数。在减少了在内存中的存取次数后，使在处理器中加速查找hash表键值成为可能。如果hash bucket size等于一路处理器缓存的大小，那么在查找键的时候，最坏的情况下在内存中查找的次数为2。第一次是确定存储单元的地址，第二次是在存储单元中查找键 值。因此，如果Nginx给出需要增大hash max size 或 hash bucket size的提示，那么首要的是增大前一个参数的大小. server_names_hash_bucket_size 128; #客户端请求头部的缓冲区大小。这个可以根据你的系统分页大小来设置，一般一个请求的头部大小不会超过1k，不过由于一般系统分页都要大于1k，所以这里设置为分页大小。分页大小可以用命令getconf PAGESIZE取得。 client_header_buffer_size 32k; #客户请求头缓冲大小。nginx默认会用client_header_buffer_size这个buffer来读取header值，如果header过大，它会使用large_client_header_buffers来读取。 large_client_header_buffers 4 64k; #设定通过nginx上传文件的大小 client_max_body_size 8m; #开启高效文件传输模式，sendfile指令指定nginx是否调用sendfile函数来输出文件，对于普通应用设为 on，如果用来进行下载等应用磁盘IO重负载应用，可设置为off，以平衡磁盘与网络I/O处理速度，降低系统的负载。注意：如果图片显示不正常把这个改成off。 #sendfile指令指定 nginx 是否调用sendfile 函数（zero copy 方式）来输出文件，对于普通应用，必须设为on。如果用来进行下载等应用磁盘IO重负载应用，可设置为off，以平衡磁盘与网络IO处理速度，降低系统uptime。 sendfile on; #开启目录列表访问，合适下载服务器，默认关闭。 autoindex on; #此选项允许或禁止使用socke的TCP_CORK的选项，此选项仅在使用sendfile的时候使用 tcp_nopush on; tcp_nodelay on; #长连接超时时间，单位是秒 keepalive_timeout 120; #FastCGI相关参数是为了改善网站的性能：减少资源占用，提高访问速度。下面参数看字面意思都能理解。 fastcgi_connect_timeout 300; fastcgi_send_timeout 300; fastcgi_read_timeout 300; fastcgi_buffer_size 64k; fastcgi_buffers 4 64k; fastcgi_busy_buffers_size 128k; fastcgi_temp_file_write_size 128k; #gzip模块设置 gzip on; #开启gzip压缩输出 gzip_min_length 1k; #最小压缩文件大小 gzip_buffers 4 16k; #压缩缓冲区 gzip_http_version 1.0; #压缩版本（默认1.1，前端如果是squid2.5请使用1.0） gzip_comp_level 2; #压缩等级 gzip_types text/plain application/x-javascript text/css application/xml; #压缩类型，默认就已经包含textml，所以下面就不用再写了，写上去也不会有问题，但是会有一个warn。 gzip_vary on; #开启限制IP连接数的时候需要使用 #limit_zone crawler $binary_remote_addr 10m; #负载均衡配置 upstream piao.jd.com { #upstream的负载均衡，weight是权重，可以根据机器配置定义权重。weigth参数表示权值，权值越高被分配到的几率越大。 server 192.168.80.121:80 weight=3; server 192.168.80.122:80 weight=2; server 192.168.80.123:80 weight=3; #nginx的upstream目前支持4种方式的分配 #1、轮询（默认） #每个请求按时间顺序逐一分配到不同的后端服务器，如果后端服务器down掉，能自动剔除。 #2、weight #指定轮询几率，weight和访问比率成正比，用于后端服务器性能不均的情况。 #2、ip_hash #每个请求按访问ip的hash结果分配，这样每个访客固定访问一个后端服务器，可以解决session的问题。 #例如： #upstream bakend { # ip_hash; # server 192.168.0.14:88; # server 192.168.0.15:80; #} #3、fair（第三方） #按后端服务器的响应时间来分配请求，响应时间短的优先分配。 #upstream backend { # server server1; # server server2; # fair; #} #4、url_hash（第三方） #按访问url的hash结果来分配请求，使每个url定向到同一个后端服务器，后端服务器为缓存时比较有效。 #例：在upstream中加入hash语句，server语句中不能写入weight等其他的参数，hash_method是使用的hash算法 #upstream backend { # server squid1:3128; # server squid2:3128; # hash $request_uri; # hash_method crc32; #} #tips: #upstream bakend{#定义负载均衡设备的Ip及设备状态}{ # ip_hash; # server 127.0.0.1:9090 down; # server 127.0.0.1:8080 weight=2; # server 127.0.0.1:6060; # server 127.0.0.1:7070 backup; #} #在需要使用负载均衡的server中增加 proxy_pass http://bakend/; #每个设备的状态设置为: #1.down表示单前的server暂时不参与负载 #2.weight为weight越大，负载的权重就越大。 #3.max_fails：允许请求失败的次数默认为1.当超过最大次数时，返回proxy_next_upstream模块定义的错误 #4.fail_timeout:max_fails次失败后，暂停的时间。 #5.backup： 其它所有的非backup机器down或者忙的时候，请求backup机器。所以这台机器压力会最轻。 #nginx支持同时设置多组的负载均衡，用来给不用的server来使用。 #client_body_in_file_only设置为On 可以讲client post过来的数据记录到文件中用来做debug #client_body_temp_path设置记录文件的目录 可以设置最多3层目录 #location对URL进行匹配.可以进行重定向或者进行新的代理 负载均衡 } #虚拟主机的配置 server { #监听端口 listen 80; #域名可以有多个，用空格隔开 server_name www.jd.com jd.com; index index.html index.htm index.php; root /data/www/jd; #对******进行负载均衡 location ~ .*.(php|php5)?$ { fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; include fastcgi.conf; } #图片缓存时间设置 location ~ .*.(gif|jpg|jpeg|png|bmp|swf)$ { expires 10d; } #JS和CSS缓存时间设置 location ~ .*.(js|css)?$ { expires 1h; } #日志格式设定 #$remote_addr与$http_x_forwarded_for用以记录客户端的ip地址； #$remote_user：用来记录客户端用户名称； #$time_local： 用来记录访问时间与时区； #$request： 用来记录请求的url与http协议； #$status： 用来记录请求状态；成功是200， #$body_bytes_sent ：记录发送给客户端文件主体内容大小； #$http_referer：用来记录从那个页面链接访问过来的； #$http_user_agent：记录客户浏览器的相关信息； #通常web服务器放在反向代理的后面，这样就不能获取到客户的IP地址了，通过$remote_add拿到的IP地址是反向代理服务器的iP地址。反向代理服务器在转发请求的http头信息中，可以增加x_forwarded_for信息，用以记录原有客户端的IP地址和原来客户端的请求的服务器地址。 log_format access '$remote_addr - $remote_user [$time_local] &quot;$request&quot; ' '$status $body_bytes_sent &quot;$http_referer&quot; ' '&quot;$http_user_agent&quot; $http_x_forwarded_for'; #定义本虚拟主机的访问日志 access_log /usr/local/nginx/logs/host.access.log main; access_log /usr/local/nginx/logs/host.access.404.log log404; #对 &quot;/&quot; 启用反向代理 location / { proxy_pass http://127.0.0.1:88; proxy_redirect off; proxy_set_header X-Real-IP $remote_addr; #后端的Web服务器可以通过X-Forwarded-For获取用户真实IP proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; #以下是一些反向代理的配置，可选。 proxy_set_header Host $host; #允许客户端请求的最大单文件字节数 client_max_body_size 10m; #缓冲区代理缓冲用户端请求的最大字节数， #如果把它设置为比较大的数值，例如256k，那么，无论使用firefox还是IE浏览器，来提交任意小于256k的图片，都很正常。如果注释该指令，使用默认的client_body_buffer_size设置，也就是操作系统页面大小的两倍，8k或者16k，问题就出现了。 #无论使用firefox4.0还是IE8.0，提交一个比较大，200k左右的图片，都返回500 Internal Server Error错误 client_body_buffer_size 128k; #表示使nginx阻止HTTP应答代码为400或者更高的应答。 proxy_intercept_errors on; #后端服务器连接的超时时间_发起握手等候响应超时时间 #nginx跟后端服务器连接超时时间(代理连接超时) proxy_connect_timeout 90; #后端服务器数据回传时间(代理发送超时) #后端服务器数据回传时间_就是在规定时间之内后端服务器必须传完所有的数据 proxy_send_timeout 90; #连接成功后，后端服务器响应时间(代理接收超时) #连接成功后_等候后端服务器响应时间_其实已经进入后端的排队之中等候处理（也可以说是后端服务器处理请求的时间） proxy_read_timeout 90; #设置代理服务器（nginx）保存用户头信息的缓冲区大小 #设置从被代理服务器读取的第一部分应答的缓冲区大小，通常情况下这部分应答中包含一个小的应答头，默认情况下这个值的大小为指令proxy_buffers中指定的一个缓冲区的大小，不过可以将其设置为更小 proxy_buffer_size 4k; #proxy_buffers缓冲区，网页平均在32k以下的设置 #设置用于读取应答（来自被代理服务器）的缓冲区数目和大小，默认情况也为分页大小，根据操作系统的不同可能是4k或者8k proxy_buffers 4 32k; #高负荷下缓冲大小（proxy_buffers*2） proxy_busy_buffers_size 64k; #设置在写入proxy_temp_path时数据的大小，预防一个工作进程在传递文件时阻塞太长 #设定缓存文件夹大小，大于这个值，将从upstream服务器传 proxy_temp_file_write_size 64k; } #设定查看Nginx状态的地址 location /NginxStatus { stub_status on; access_log on; auth_basic &quot;NginxStatus&quot;; auth_basic_user_file confpasswd; #htpasswd文件的内容可以用apache提供的htpasswd工具来产生。 } #本地动静分离反向代理配置 #所有jsp的页面均交由tomcat或resin处理 location ~ .(jsp|jspx|do)?$ { proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_pass http://127.0.0.1:8080; } #所有静态文件由nginx直接读取不经过tomcat或resin location ~ .*.(htm|html|gif|jpg|jpeg|png|bmp|swf|ioc|rar|zip|txt|flv|mid|doc|ppt| pdf|xls|mp3|wma)$ { expires 15d; } location ~ .*.(js|css)?$ { expires 1h; } } } ######Nginx配置文件nginx.conf中文详解##### ","link":"https://esp0x.github.io/post/nginx-pei-zhi-wen-jian-xiang-jie/"},{"title":"nc命令使用","content":"端口扫描 # nc -vz -w 5 127.0.0.1 1-1024 // 扫描本地1-1024端口范围 监听端口 # nc -l 8000 // 监听TCP端口 # nc -ul 9999 // 监听UDP端口 # nc -vuz 127.0.0.1 9999 // 测试本地UDP端口 传输文件 1.上传 # nc -l 9999 &gt; filename # nc 127.0.0.1 9999 &lt; source 2.下载 # nc -l 9999 &lt; filename # nc 127.0.0.1 9999 &gt; target ","link":"https://esp0x.github.io/post/nc-ming-ling-shi-yong/"},{"title":"StorCLI工具使用说明","content":"记录一下storcli工具的基本查询方法，主要用于实现监控目的，对于更改Raid的相关命令暂时不在这里记录。 获取帮助信息 # ./storcli64 -h Storage Command Line Tool Ver 1.23.02 Mar 28, 2017 (c)Copyright 2017, AVAGO Corporation, All Rights Reserved. storcli -v storcli -h| -help| ? storcli -h| -help| ? legacy storcli show storcli show all storcli show ctrlcount storcli show file=&lt;filepath&gt; storcli /cx add vd r[0|1|5|6|00|10|50|60] ... 显示Raid卡相关信息 # ./storcli64 show all Status Code = 0 Status = Success Description = None Number of Controllers = 1 Host Name = A-f8f21e93eca4-nas014 Operating System = Linux3.10.0-1062.el7.x86_64 System Overview : =============== ------------------------------------------------------------------------------------- Ctl Model Ports PDs DGs DNOpt VDs VNOpt BBU sPR DS EHS ASOs Hlth ------------------------------------------------------------------------------------- 0 AVAGOMegaRAIDSAS9361-8i 8 25 2 1 2 1 Opt On 1&amp;2 Y 3 NdAtn ------------------------------------------------------------------------------------- Ctl=Controller Index|DGs=Drive groups|VDs=Virtual drives|Fld=Failed PDs=Physical drives|DNOpt=DG NotOptimal|VNOpt=VD NotOptimal|Opt=Optimal Msng=Missing|Dgd=Degraded|NdAtn=Need Attention|Unkwn=Unknown sPR=Scheduled Patrol Read|DS=DimmerSwitch|EHS=Emergency Hot Spare Y=Yes|N=No|ASOs=Advanced Software Options|BBU=Battery backup unit Hlth=Health|Safe=Safe-mode boot # 这里的optimal表示是否是最优状态 显示控制器信息 # ./storcli64 /c0 show //这里c0表示第一个控制器 Generating detailed summary of the adapter, it may take a while to complete. Controller = 0 ...省略 Device Number = 0 Function Number = 0 Drive Groups = 2 TOPOLOGY : ======== ----------------------------------------------------------------------------- DG Arr Row EID:Slot DID Type State BT Size PDC PI SED DS3 FSpace TR ----------------------------------------------------------------------------- ... 省略 1 0 12 10:12 37 DRIVE Onln Y 14.551 TB dflt N N dflt - N 1 0 13 10:13 36 DRIVE Onln Y 14.551 TB dflt N N dflt - N 1 0 14 10:14 46 DRIVE Onln Y 14.551 TB dflt N N dflt - N 1 0 15 - - DRIVE Msng - 14.551 TB - - - - - N 1 0 16 10:16 38 DRIVE Onln Y 14.551 TB dflt N N dflt - N 1 0 17 10:17 35 DRIVE Onln Y 14.551 TB dflt N N dflt - N 1 0 18 10:18 45 DRIVE Onln Y 14.551 TB dflt N N dflt - N ----------------------------------------------------------------------------- ... 省略 Missing Drives Count = 1 # 这里提示我们有一块盘丢失，需要进行处理 显示剩余空间 # ./storcli64 /c0 show freespace 显示CC（Consistency Check） # ./storcli64 /c0 show cc Controller = 0 Status = Success Description = None Controller Properties : ===================== ----------------------------------------------- Ctrl_Prop Value ----------------------------------------------- CC Operation Mode Concurrent CC Execution Delay 168 CC Next Starttime 02/04/2021, 15:00:00 CC Current State Stopped CC Number of iterations 15 CC Number of VD completed 1 CC Excluded VDs None ----------------------------------------------- 显示CC速率 # ./storcli64 /c0 show ccrate Controller = 0 Status = Success Description = None Controller Properties : ===================== ---------------- Ctrl_Prop Value ---------------- CC Rate 30% ---------------- 查看与设置Rebuild速率 # ./storcli64 /c0 show rebuildrate //查看速率 # ./storcli64 /c0 set rebuildrate=30 //设置速率 清除Raid卡，物理磁盘Cache # ./storcli64 /c0 flushcache 获取所有enclosure信息 # ./storcli64 /c0/eall show 获取单个enclosure信息 # ./storcli64 /c0/e10 show &lt;all&gt; //加上all参数表示获取详细信息 # ./strocli64 /c0/e10 show status //获取风扇等设备详细信息 获取所有磁盘详细信息 # ./storcli64 /c0/eall/sall show 卷组信息获取 # ./storcli64 /c0/dall show // 这里的卷组称为DG 参考文献 https://www.cnblogs.com/luxiaodai/p/9878747.html ","link":"https://esp0x.github.io/post/storcli-gong-ju-shi-yong-shuo-ming/"},{"title":"Mysql的组提交原理","content":"事务提交流程 大致流程如下： 有binlog的情况下，commit动作开始时，会有一个Redo XID写入redo，然后写data到binlog，binlog写成功后，会将binlog的filename和日志写的position再写回redo（position也会写入pos文件），此时事务完成（committed）。如果只有XID，没有filename和position，则表示事务为prepare状态。 流程： # commit; --&gt; write XID to redo. --&gt; write data to Binlog. --&gt; write filename,postsion of binlog to redo. --&gt; commited. # 记录Binlog是在InnoDB引擎Prepare（即Redo Log写入磁盘）之后，这点至关重要。 不同阶段crash的情况： crash发生阶段 事务状态 事务结果 当事务在prepare阶段crash 该事务未写入binlog，引擎层也未写入redo到磁盘 该事务rollback 当事务在binlog写阶段crash 此时引擎层redo写盘完成，但binlog日志还未落盘 该事务rollback 当事务在binlog日志写入磁盘后crash，但引擎层未来得及commit 此时引擎层redo已经写盘，server层binlog已经写盘，但redo中事务状态未正确结束 读出binlog中的XID，并通知引擎层提交这些XID的事务。引擎层提交这些事务后，会回滚其他事务，使引擎层redo和binlog日志在事务上始终保持一致。事务通过recovery自动完成提交 WAL机制 WAL（Write Ahead Log）：对数据文件进行修改前，必须将修改先记录到日志。 Redo log就是一种WAL应用，用于保证数据库的持久性，每次事务提交时，不用同步刷新磁盘，只需要刷新redo log就行了。相比刷盘的随机IO，写redo log的顺序IO能够提升事务提交速度。 组提交： 未开启binlog：redo log的刷盘操作是主要瓶颈，mysql使用组提交，将多个redo log刷盘操作合并成一个。 开启binlog：为了保证redo log和binlog数据一致性，mysql使用了二阶段提交，此时binlog成为瓶颈，mysql增加了binlog的组提交来解决这个问题，分为三个阶段（Flush、Sync、Commit），最大化刷盘收益。 过程详解 在Mysql中每个阶段都有一个队列，每个队列都有一把锁保护，第一个进入队列的事务成为leader，leader领导所在队列的所有事务，全权负责整队的操作，完成后通知队内其他事务操作结束。 Flush阶段 首先获取队列中的事务组； 将redo中prepare阶段的数据刷盘； 将binlog数据写入文件，此处是文件缓冲，不保证数据库crash时，binlog的完整性； Flush阶段的作用是提供了redo 的组提交； 如果这一步crash，由于不保证binlog中存在事务记录，所以数据库恢复后会回滚，此时二阶段提交状态还是prepare； Sync阶段 为了增加一组事务中的事务数量，提升刷盘效率，使用两个参数进行控制： binlog_group_commit_sync_delay=N 等待N 微秒后，开始事务刷盘 binlog_group_commit_sync_no_delay=N 对列中事务达到N个，立刻刷盘，忽略上面那个时间参数 Sync阶段的作用是支持binlog的组提交； 如果此时crash，由于binlog中有事务记录，数据库恢复后会继续提交该事务； Commit阶段 首先获取队列中的事务组； 依次将redo中已经prepare的事务在引擎层进行提交； Commit阶段不用刷盘，如上所述，Flush阶段中的Redo log刷盘已经足够保证数据库崩溃时的数据安全了； Commit阶段队列的作用是承接Sync阶段的事务，完成最后的引擎提交，使得Sync可以尽早的处理下一组事务，最大化组提交的效率； ","link":"https://esp0x.github.io/post/mysql-de-zu-ti-jiao-yuan-li/"},{"title":"GTID基础原理","content":"一、GTID概述 GTID是MYSQL5.6新增的特性，GTID（Global Transaction Identifier）全称为全局事务标示符,用以数据库实例事务唯一标识，其组成主要是source_id和transaction_id 即GTID = source_id:transaction_id。其中source_id是数据库启动自动生成的数据库实例唯一标识，保存在auto.cnf中，而transaction_id则是事务执行的序列号。 二、GTID优缺点 优点： 复制安全性更高，一个事务在每个实例上只执行一次； 故障切换简单，可通过设置MASTER_AUTO_POSITION=1，而非master_log_file和master_log_pos来建立主从关系； 可根据GTID确定事务最早提交的实例； 缺点： 组复制中，必须要求统一开启GTID或者关闭GTID； 不支持复制create table table_name select ... from table_name_xx ; 不支持create temporary table和drop temporary table； 不支持sql_slave_skip_counter，可通过set global gtid_next='' 跳过； 从库和主库都必须设置log_slave_updates 三、GTID工作原理 1、master更新数据时，会在事务前产生GTID，一同记录到binlog日志中。 2、slave端的i/o 线程将变更的binlog，写入到本地的relay log中。 3、sql线程从relay log中获取GTID，然后对比slave端的binlog是否有记录。 4、如果有记录，说明该GTID的事务已经执行，slave会忽略。 5、如果没有记录，slave就会从relay log中执行该GTID的事务，并记录到binlog。 6、在解析过程中会判断是否有主键，如果没有就用二级索引，如果没有就用全部扫描。 四、GTID开启和关闭 gtid_mode=ON(必选) log_bin=ON(必选) log-slave-updates=ON(必选) enforce-gtid-consistency(必选) log-bin = /home/mysql/mysql-bin（必选） binlog_format = MIXED（必选mixed或者row） ## change master to master_host = 'ipaddr',master_port = 3306,master_user = 'username',master_password='password',master_auto_position = 1; 五、GTID适用场景 1、搭建高可用架构，方便主从切换后，新的从库重新指定主库（例如一主二从的结构，A为mater,B为Slave，C为Slave，A宕机切换到B后，C重新指定主库为B） 2、不经常使用create table table_name select * from table_name/create temporary table/update t1,t2 where ...这种语句的场合 ","link":"https://esp0x.github.io/post/gtid-ji-chu-yuan-li/"},{"title":"Mysql事务隔离级别及实现","content":"四个基本要素（ACID） 原子性（Atomicity） 一致性（Consistency） 隔离性（Isolation） 持久性（Durability） 事务的并发问题 脏读：事务A读取了事务B更新的数据，然后B回滚操作，那么A读到的数据是脏数据； 不可重复读：事务A多次读取同一数据，事务B在事务A多次读取的过程中，对数据作了更新并提交，导致事务A多次读取的数据不一致； 幻读：幻读并不是说两次读取的结果集不同，幻读侧重的方面是某一次的select操作得到的结果所表征的数据状态无法支撑后续的业务操作。具体来说，select某条记录是否存在，不存在，准备插入此记录，但执行insert时发现此记录已存在，无法插入，此时就发生了幻读。 事务隔离级别 事务隔离级别 脏读 不可重复读 幻读 读未提交（read-uncommitted） 是 是 是 不可重复读（read-committed） 否 是 是 可重复读（repeatable-read） 否 否 是 串行化（serializable） 否 否 否 查看数据库隔离级别 mysql&gt; select @@global.tx_isolation, @@tx_isolation; ACID实现原理 原子性 实现原子性的关键，是当事务回滚时能够撤销所有已经成功执行的sql语句。InnoDB实现回滚，依赖的是undo log：当事务对数据库进行修改时，InnoDB会生成对应的undo log，如果事务执行失败或调用rollback，导致事务需要回滚，便可以利用undo log中的信息将数据进行回滚。undo log其实就是记录数据修改时的相反操作。 持久性 实现持久性主要依赖redo log，redo log采用WAL，所有修改先写入日志，再更新到Buffer Pool，保证了数据库不会因意外宕机而丢失。写redo log是要比刷缓存到磁盘要快的，原因如下： 刷脏是随机IO、这个很好理解；而redo log是追加操作，属于顺序IO，所以速度快； 刷脏是以Page为单位的，Mysql默认页大小是16KB，一个Page上一个小的修改都需要整页写入；而redo log中只包含真正需要写入的部分，无效IO减少； 关于binlog和redo log的区别： 作用不同：redo log是用于crash recovery的，保证MySQL宕机也不会影响持久性；binlog是用于point-in-time recovery的，保证服务器可以基于时间点恢复数据，此外binlog还用于主从复制。 层次不同：redo log是InnoDB存储引擎实现的，而binlog是MySQL的服务器层实现的，同时支持InnoDB和其他存储引擎。 内容不同：redo log是物理日志，内容基于磁盘的Page；binlog的内容是二进制的，根据binlog_format参数的不同，可能基于sql语句、基于数据本身或者二者的混合。 写入时机不同：binlog在事务提交时写入；redo log的写入时机相对多元，可以通过双1设置进行控制； 隔离性 隔离性追求的是并发条件下事务之间互不干扰；主要考虑两种场景： 两个事务的写操作之间的影响： InnoDB通过锁机制来保证同一时刻只有一个事务进行写操作，简单理解为，事务在修改数据之前，需要先获得相应的锁；获得锁之后，事务便可以修改数据，其他事务需要等待该锁释放后才能对同一数据进行操作。 表锁会锁住整个表，并发性能较差； 行锁只锁定需要操作的数据，并发性能好，但数据较多时性能也会下降；绝大多数情况下，我们使用行锁即可； 两个事务的读操作之间的影响： Mysql中默认的隔离级别是RR，InnoDB实现的RR可以避免幻读的问题，即使用MVCC技术，Multi-Version Concurrency Control。 MVCC最大的优点是读不加锁，因此读写不冲突，并发性好。InnoDB实现MVCC，多个版本的数据可以共存，主要依赖以下技术和数据结构： 数据库需要做好版本控制，防止不该被事务看到的数据(例如还没提交的事务修改的数据)被看到。在InnoDB中，主要是通过使用readview的技术来实现判断。查询出来的每一行记录，都会用readview来判断一下当前这行是否可以被当前事务看到，如果可以，则输出，否则就利用undolog来构建历史版本，再进行判断，知道记录构建到最老的版本或者可见性条件满足。 在trx_sys中，一直维护这一个全局的活跃的读写事务id(trx_sys-&gt;descriptors)，id按照从小到大排序，表示在某个时间点，数据库中所有的活跃(已经开始但还没提交)的读写(必须是读写事务，只读事务不包含在内)事务。当需要一个一致性读的时候(即创建新的readview时)，会把全局读写事务id拷贝一份到readview本地(read_view_t-&gt;descriptors)，当做当前事务的快照。read_view_t-&gt;up_limit_id是read_view_t-&gt;descriptors这数组中最小的值，read_view_t-&gt;low_limit_id是创建readview时的max_trx_id，即一定大于read_view_t-&gt;descriptors中的最大值。当查询出一条记录后(记录上有一个trx_id，表示这条记录最后被修改时的事务id)，可见性判断的逻辑如下(lock_clust_rec_cons_read_sees)： 如果记录上的trx_id小于read_view_t-&gt;up_limit_id，则说明这条记录的最后修改在readview创建之前，因此这条记录可以被看见。 如果记录上的trx_id大于等于read_view_t-&gt;low_limit_id，则说明这条记录的最后修改在readview创建之后，因此这条记录肯定不可以被看家。 如果记录上的trx_id在up_limit_id和low_limit_id之间，且trx_id在read_view_t-&gt;descriptors之中，则表示这条记录的最后修改是在readview创建之时，被另外一个活跃事务所修改，所以这条记录也不可以被看见。如果trx_id不在read_view_t-&gt;descriptors之中，则表示这条记录的最后修改在readview创建之前，所以可以看到。 基于上述判断，如果记录不可见，则尝试使用undo去构建老的版本(row_vers_build_for_consistent_read)，直到找到可以被看见的记录或者解析完所有的undo。 针对RR隔离级别，在第一次创建readview后，这个readview就会一直持续到事务结束，也就是说在事务执行过程中，数据的可见性不会变，所以在事务内部不会出现不一致的情况。针对RC隔离级别，事务中的每个查询语句都单独构建一个readview，所以如果两个查询之间有事务提交了，两个查询读出来的结果就不一样。从这里可以看出，在InnoDB中，RR隔离级别的效率是比RC隔离级别的高。此外，针对RU隔离级别，由于不会去检查可见性，所以在一条SQL中也会读到不一致的数据。针对串行化隔离级别，InnoDB是通过锁机制来实现的，而不是通过多版本控制的机制，所以性能很差。 由于readview的创建涉及到拷贝全局活跃读写事务id，所以需要加上trx_sys-&gt;mutex这把大锁，为了减少其对性能的影响，关于readview有很多优化。例如，如果前后两个查询之间，没有产生新的读写事务，那么前一个查询创建的readview是可以被后一个查询复用的。 操作指令： mysql&gt; select * from information_schema.innodb_locks; #锁的概况 mysql&gt; show engine innodb status; #InnoDB整体状态，其中包括锁的情况 一致性 一致性是事务追求的终极目标，原子性、持久性和隔离性，都是为了实现一致性而存在的；实现一致性也需要应用层面进行保障； 运维相关指令和参数 1、首先介绍一下information_schema中的三张表: innodb_trx, innodb_locks和innodb_lock_waits。由于这些表几乎需要查询所有事务子系统的核心数据结构，为了减少查询对系统性能的影响，InnoDB预留了一块内存，内存里面存了相关数据的副本，如果两次查询的时间小于0.1秒(CACHE_MIN_IDLE_TIME_US)，则访问的都是同一个副本。如果超过0.1秒，则这块内存会做一次更新，每次更新会把三张表用到的所有数据统一更新一遍，因为这三张表经常需要做表连接操作，所以一起更新能保证数据的一致性。这里简单介绍一下innodb_trx表中的字段，另外两张表涉及到事物锁的相关信息，由于篇幅限制，后续有机会在介绍。 trx_id: 就是trx_t中的事务id，如果是只读事务，这个id跟trx_t的指针地址有关，所以可能是一个很大的数字(trx_get_id_for_print)。 trx_weight: 这个是事务的权重，计算方法就是undolog数量加上事务已经加上锁的数量。在事务回滚的时候，优先选择回滚权重小的事务，有非事务引擎参与的事务被认为权重是最大的。 trx_rows_modified：这个就是当前事务已经产生的undolog数量，每更新一条记录一次，就会产生一条undo。 trx_concurrency_tickets: 每次这个事务需要进入InnoDB层时，这个值都会减一，如果减到0，则事务需要等待(压力大的情况下)。 trx_is_read_only: 如果是以start transaction read only启动事务的，那么这个字段是1，否则为0。 trx_autocommit_non_locking: 如果一个事务是一个普通的select语句(后面没有跟for update, share lock等)，且当时的autocommit为1，则这个字段为1，否则为0。 trx_state: 表示事务当前的状态，只能有RUNNING, LOCK WAIT, ROLLING BACK, COMMITTING这几种状态, 是比较粗粒度的状态。 trx_operation_state: 表示事务当前的详细状态，相比于trx_state更加详细，例如有rollback to a savepoint, getting list of referencing foreign keys, rollback of internal trx on stats tables, dropping indexes等。 2、与事务相关的undo参数 innodb_undo_directory: undo文件的目录，建议放在独立的一块盘上，尤其在经常有大事务的情况下。 innodb_undo_logs: 这个是定义了undo segment的个数。在给读写事务分配undo segment的时候，拿这个值去做轮训分配。 Innodb_available_undo_logs: 这个是一个status变量，在启动的时候就确定了，表示的是系统上分配的undo segment。举个例子说明其与innodb_undo_logs的关系：假设系统初始化的时候innodb_undo_logs为128，则在文件上一定有128个undo segment，Innodb_available_undo_logs也为128，但是启动起来后，innodb_undo_logs动态被调整为100，则后续的读写事务只会使用到前100个回滚段，最后的20多个不会使用。 innodb_undo_tablespaces: 存放undo segment的物理文件个数，文件名为undoN，undo segment会比较均匀的分布在undo tablespace中。 3、与Purge相关的参数 innodb_purge_threads: Purge Worker和Purge Coordinator总共的个数。在实际的实现中，使用多少个线程去做Purge是InnoDB根据实时负载进行动态调节的。 innodb_purge_batch_size: 一次性处理的undolog的数量，处理完这个数量后，Purge线程会计算是否需要sleep。 innodb_max_purge_lag: 如果全局历史链表超过这个值，就会增加Purge Worker线程的数量，也会使用sleep的方式delay用户的DML。 innodb_max_purge_lag_delay: 这个表示通过sleep方式delay用户DML最大的时间。 4、与回滚相关的参数 innodb_lock_wait_timeout: 等待行锁的最大时间，如果超时，则会滚当前语句或者整个事务。发生回滚后返回类似错误：Lock wait timeout exceeded; try restarting transaction。 innodb_rollback_on_timeout: 如果这个参数为true，则当发生因为等待行锁而产生的超时时，回滚掉整个事务，否则只回滚当前的语句。这个就是隐式回滚机制。主要是为了兼容之前的版本。 总结 原子性：语句要么全执行，要么全不执行，是事务最核心的特性，事务本身就是以原子性来定义的；实现主要基于undo log 持久性：保证事务提交后不会因为宕机等原因导致数据丢失；实现主要基于redo log 隔离性：保证事务执行尽可能不受其他事务影响；InnoDB默认的隔离级别是RR，RR的实现主要基于锁机制（包含next-key lock）、MVCC（包括数据的隐藏列、基于undo log的版本链、ReadView） 一致性：事务追求的最终目标，一致性的实现既需要数据库层面的保障，也需要应用层面的保障 参考文献 http://mysql.taobao.org/monthly/2017/12/01/ https://www.cnblogs.com/kismetv/p/10331633.html https://zhuanlan.zhihu.com/p/40208895 https://www.cnblogs.com/huanongying/p/7021555.html ","link":"https://esp0x.github.io/post/mysql-shi-wu-ge-chi-ji-bie-ji-shi-xian/"},{"title":"Mysql主从复制环境搭建","content":"准备机器 两台centos7系统服务器： 主mysql：192.168.50.1 从mysql：192.168.50.2 安装mysql 下载yum源 wget https://dev.mysql.com/get/mysql80-community-release-el7-3.noarch.rpm 安装yum源 rpm -Uvh mysql80-community-release-el7-3.noarch.rpm 安装mysql yum install -y mysql-community-server 启动mysql，并获取root初始密码 # 确保selinux为disabled # 确保关闭了firewalld服务 systemctl start mysqld [root@localhost ~]# grep password /var/log/mysqld.log 2020-10-19T02:47:35.912896Z 1 [Note] A temporary password is generated for root@localhost: -1k-1KjU*LG) 登入mysql，并重置root密码 mysql -uroot -p mysql&gt; ALTER USER 'root'@'localhost' IDENTIFIED BY 'qUXmZP11JwJ_11'; # root本地访问，且重置 mysql&gt; exit 主Mysql数据库配置 编辑my.cnf配置文件 vim /etc/my.cnf [mysqld] port=9006 #指定新的端口 server-id=110 #设置主服务器的ID(不能和别的服务器重复，建议使用ip的最后一段) log-bin=mysql-bin #binlog日志文件名 创建用于主从同步的账户 $ mysql -u root -p #登录MySQL mysql&gt; CREATE USER 'repl'@'192.168.50.2' IDENTIFIED WITH mysql_native_password BY 'Top_master_1'; # 主库创建用于从库同步的账号 mysql&gt; grant replication slave on *.* to 'repl'@'192.168.50.2'; #赋予主从同步权限，指定具体的数据库在/etc/my.cnf中完成 mysql&gt; flush privileges; 重启MySQL，使my.cnf 配置生效；查看主库状态 $ systemctl restart mysqld #重启MySQL mysql -u root -p mysql&gt; show master status; #查看主库的状态 File,Position 这两个值需要放到slave配置中 +--------------------+----------+--------------+------------------+-------------------+ | File | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set | +--------------------+----------+--------------+------------------+-------------------+ | mysql-bin.00001 | 156 | xxxx | | | +--------------------+----------+--------------+------------------+-------------------+ 1 row in set (0.00 sec) 从Mysql数据库配置 编辑配置文件 vim /etc/my.cnf [mysqld] port=9006 server-id=111 配置完成后，重启从库的MySQL $ systemctl restart mysqld #重启MySQL $ mysql -u root -p #登录mysql mysql&gt; stop slave; #关闭从库 mysql&gt; change master to master_host='192.168.50.1',master_port=9006,master_user='repl',master_password='Top_master_1',master_log_file='mysql-bin.00001',master_log_pos=156; #配置主库信息 mysql&gt; start slave; #开启从库 mysql&gt; show slave status \\G; #Slave_IO_Running,Slave_SQL_Running 都为Yes的时候表示配置成功 主库创建数据库和数据表 create database topmanager DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci; 登入从库，可以看到从库中出现同样的数据库和数据表，表明已同步 ","link":"https://esp0x.github.io/post/mysql-zhu-cong-fu-zhi-huan-jing-da-jian/"},{"title":"Mysql双1设置-数据安全的关键参数","content":"一、参数及设置说明 # innodb_flush_log_at_trx_commit 1.设置为0：log buffer将每秒一次地写入log file中，并且log file的flush操作同时进行；该模式下，在事务提交时，不会主动触发写入磁盘的操作； 2.设置为1：每次事务提交时MySQL都会把log buffer的数据写入log file，并且flush(刷到磁盘)中去; 3.设置为2：每次事务提交时MySQL都会把log buffer的数据写入log file，但是flush(刷到磁盘)操作并不会同时进行。该模式下,MySQL会每秒执行一次 flush(刷到磁盘)操作。 # sync_binlog sync_binlog 的默认值是0，像操作系统刷其他文件的机制一样，MySQL不会同步到磁盘中去而是依赖操作系统来刷新binary log。 当sync_binlog =N (N&gt;0) ，MySQL 在每写 N次 二进制日志binary log时，会使用fdatasync()函数将它的写二进制日志binary log同步到磁盘中去。 注意：如果启用了autocommit，那么每一个语句statement就会有一次写操作；否则每个事务对应一个写操作。 # 性能与安全性对比 1.双1设置时，写入性能最差，安全性最高； 2.sync_binlog=N (N&gt;1 ) innodb_flush_log_at_trx_commit=2 时，性能最好，安全性较差； 3.innodb_flush_log_at_trx_commit设置为0，mysqld进程崩溃会导致上一秒钟所有事务数据的丢失； 4.innodb_flush_log_at_trx_commit设置为2，当系统崩溃或者断电时，上一秒的所有数据才有可能丢失； 二、对IO影响较大的几个参数 1.innodb_buffer_pool_size # 该参数控制innodb缓存大小，用于缓存应用访问的数据，推荐配置为系统可用内存的80%。 2.binlog_cache_size # 该参数控制二进制日志缓冲大小，当事务还没有提交时，事务日志存放于cache，当遇到大事务cache不够用的时，mysql会把uncommitted的部分写入临时文件,等到committed的时候才会写入正式的持久化日志文件。 3.innodb_max_dirty_pages_pct # 该参数可以直接控制Dirty Page在BP中所占的比率，当dirty page达到了该参数的阈值，就会触发MySQL系统刷新数据到磁盘。 4.innodb_flush_log_at_trx_commit # 该参数确定日志文件何时write、flush。 5.sync_binlog # sync_binlog的默认值是0，像操作系统刷其他文件的机制一样，MySQL不会同步到磁盘中去而是依赖操作系统来刷新binary log。 6.innodb_flush_method # 该参数控制日志或数据文件如何write、flush。可选的值为fsync，o_dsync，o_direct，littlesync，nosync。 ","link":"https://esp0x.github.io/post/mysql-shuang-1-she-zhi-shu-ju-an-quan-de-guan-jian-can-shu/"},{"title":"Mysql分库分表策略","content":"数据切分就是将数据分散存储到多个数据库中，使得单一数据库中的数据量变小，通过扩充主机的数量缓解单一数据库的性能问题，从而达到提升数据库性能的目的。 垂直切分 垂直切分常见有垂直分库和垂直分表两种： 垂直分库就是根据业务耦合性，将关联度低的不同表存储在不同的数据库中。类似微服务架构，每一个微服务单独使用一个数据库的形式。 垂直分表是基于数据库中的列进行， 针对某个表字段过多，可以新建一张扩展表，将不常用的字段或长度较大的字段拆分到扩展表中。这样可以避免跨页问题，MySQL底层是通过数据页存储的，一条记录过大会导致跨页，造成额外的性能损失。 垂直切分的优点： 解决业务系统层面的耦合，使得业务逻辑更清晰； 与微服务的治理类似，也能对不同业务的数据进行分级管理、维护、监控、扩展等； 高并发场景下，垂直切分一定程度上能够提升访问性能； 垂直切分的缺点： 部分表无法join，只能通过接口方式，开发难度增加； 分布式事务处理复杂； 没有解决单表数据过大的问题； 水平切分 当一个应用难以再细粒化的垂直切分，或切分后单表数据量过大，存在读写性能问题时，就需要考虑水平切分了。 水平切分包括库内分表和分库分表，库内分表只解决了单一表数据量过大的问题，没有将表分布到不同的机器上，对于数据库访问性能提升有限。 水平切分的优点： 不存在单表数据量过大问题，提升了表的访问性能； 应用端改造较小，不需要进行业务拆分； 水平切分的缺点： 跨分片事务的一致性难以保证； 跨库的join关联查询性能较差； 数据库多次扩展难度和维护量增加； 几种典型的数据分片规则 根据数值范围 例如：按照时间维度，将不同月，甚至不同日的数据存储到不同的表；又或者，按照userId进行划分，1-9999的记录分到第一个库，10000-20000的分到第二个库，以此类推； 优点： 单表大小可控； 天然便于水平扩展，后期如果想对整个分片集群扩容时，只需要添加节点，无需对其他分片进行数据迁移； 使用分片字段进行查询时，速度更快； 缺点： 热点数据成为性能瓶颈。连续分片可能存在数据热点，例如按时间字段分片，有些分片存储最近时间的数据，可能会被频繁地读写，有些分片存储历史数据，则很少被查询。 根据数值取模 一般采用hash取模mod的切分方式。 优点： 数据分片相对比较均匀，不容易出现热点和并发访问的瓶颈； 缺点： 后期扩展时，需要迁移旧的数据；（通过一致性hash算法可以避免这个问题） 容易面临跨分片查询的复杂问题，比如频繁查询中不包含分片字段，将会导致无法定位数据库，从而向所有数据库发起请求，再在内存中合并数据，性能损失严重； 分库分表需要解决的问题 事务问题 方案一：使用分布式事务，交由数据库管理，简单有效；缺点是随着节点增加，性能代价越来越高。（需要协调的节点变多） 方案二：由应用程序和数据库共同控制，性能上有优势；缺点是开发难度较大； 跨节点Join问题 分两次查询，第一次查询的结果集中找出关联数据的ID，根据这些ID发起第二次请求得到关联数据；在应用设计时应尽量避免进行关联查询； 跨节点的count、order by、group by以及聚合函数问题 分别在各个节点进行数据合并，最后在统一进行合并，缺点是当数据集较大时，占用的内存资源很多； 数据迁移、容量规划、扩容等问题 当业务高速发展，面临性能和存储的瓶颈时，才会考虑分片设计，此时就不可避免的需要考虑历史数据迁移的问题。一般做法是先读出历史数据，然后按指定的分片规则再将数据写入到各个分片节点中。此外还需要根据当前的数据量和QPS，以及业务发展的速度，进行容量规划，推算出大概需要多少分片（一般建议单个分片上的单表数据量不超过1000W）如果采用数值范围分片，只需要添加节点就可以进行扩容了，不需要对分片数据迁移。如果采用的是数值取模分片，则考虑后期的扩容问题就相对比较麻烦。 全局主键避重问题 由于表同时存在于多个数据库中，主键值设置为自增序列将不能使用，需要单独设计全局主键，一般可以用UUID的方案解决； 跨分片的排序分页 一般来讲，分页时需要按照指定字段进行排序。当排序字段就是分片字段的时候，我们通过分片规则可以比较容易定位到指定的分片，而当排序字段非分片字段的时候，情况就会变得比较复杂了。为了最终结果的准确性，我们需要在不同的分片节点中将数据进行排序并返回，并将不同分片返回的结果集进行汇总和再次排序，最后再返回给用户。 如果是在前台应用提供分页，则限定用户只能看前面n页，这个限制在业务上也是合理的，一般看后面的分页意义不大（如果一定要看，可以要求用户缩小范围重新查询）。 如果是后台批处理任务要求分批获取数据，则可以加大page size，比如每次获取5000条记录，有效减少分页数（当然离线访问一般走备库，避免冲击主库）。 分库设计时，一般还有配套大数据平台汇总所有分库的记录，有些分页查询可以考虑走大数据平台。 ","link":"https://esp0x.github.io/post/mysql-fen-ku-fen-biao-ce-lue/"},{"title":"Mysql主从复制原理","content":"原理图如下： 主从复制流程详解： master库发生数据改变时，会将改变写入binglog日志； slave库会在一定时间间隔内对master的binlog进行检测以确定是否发生改变，如果发生改变，则开始一个IO thread请求master二进制事件； 同时，master为每一个过来请求的IO thread开启一个dump线程，用于将二进制事件发送给IO thread，slave的IO thread将此二进制事件写入本地relay log（中继日志）中，并开启SQL thread从中继日志中读取二进制日志，在本地进行重放（replay），从而使得本地数据与master保持一致；最后IO thread和SQL thread进入睡眠状态，等待下次唤醒。 主从复制形式： 主从 主主 一主多从 多主一从 联级复制 主从同步延时分析： master对所有DDL和DML产生的日志都写入binlog，由于是顺序写，所以效率很高；反之，slave的SQL thread进行relay log的重放时，DML和DDL的IO是随机的，不是顺序，所以成本较高，这里会增加一部分延时； 由于SQL thread是单线程的，当master的并发较高时，过多的DML可能会导致slave的SQL thread来不及处理，这里会增加一部分延时； slave中有部分SQL产生了锁等待，这种情况就是slave有一些读请求与重放请求产生了锁冲突导致的，也会增加延时； slave在充当读库角色的时候，如果查询访问压力过大，会消耗部分系统资源，影响同步效率； 大事务执行，即当master有大事务执行时，比如执行了10分钟，binlog写入必须要等待事务处理完毕，那么slave开始进行同步的时候就已经延时10分钟了； 延时解决办法： 业务层实现读写分离，一主多从，主写从读，分散压力； 业务层和数据库层之间加入缓存策略，降低直接对数据库的读压力；频繁写的场景不适合加缓存，会导致缓存命中降低； 升级硬件； MTS问题，即多线程的slave，从5.6版本开始支持，针对不同粒度（库、表、行）设置并行同步； 5.7版本后的并行复制策略 Redo log的两阶段提交 先写redo，再写binlog：假设在redo写完，binlog还没有写完的时候，Mysql进程异常重启，这时仍然能够通过redo log恢复数据，但由于binlog没有这条记录，所以之后备份日志的时候，binlog是缺失这条记录的，以后需要用binlog恢复数据时，就会缺少一条数据的更新； 先写binlog，再写redo log：如果binlog写完后crash，由于redo log还没写，崩溃恢复后这个事务无效，但是binlog有记录，以后用这个binlog恢复数据时，就会多出一条更新记录； 二阶段提交：使用二阶段提交时，会综合redo和binlog的状态进行处理，如果写入binlog之前crash，那么由于redo处于prepare阶段，只需要对当前事务进行回滚即可；如果写入binlog之后crash，那么由于redo处于prepare阶段，只需要对当前事务进行提交即可。 并行复制的思想 同时处于prepare状态的事务，在备库执行是可以并行的； 处于prepare状态的事务，与处于commit状态的事务之间，在备库执行也是可以并行的； binlog_group_commit_sync_delay参数，表示延迟多少微妙后才调用fsync； binlog_group_commit_sync_no_delay_count参数，表示累积多少次以后才调用fsync； ","link":"https://esp0x.github.io/post/mysql-zhu-cong-fu-zhi-yuan-li/"}]}